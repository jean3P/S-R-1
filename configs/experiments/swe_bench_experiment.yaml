# configs/experiments/swe_bench_experiment.yml

name: "swe_bench_experiment"
description: "Experiment: SWE-bench Lite evaluation"
agent:
  id: "code_refinement"
model:
  id: "qwen_coder"  # Could be any of the models
prompt:
  id: "swe_bench_prompt"
evaluator:
  id: "swe_bench_eval"
task:
  name: "swe_bench_task"
  language: "python"
  initial_prompt: "This is a placeholder. The actual prompt will be generated from the SWE-bench dataset."
