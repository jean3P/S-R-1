id: "qwen_coder"
type: "huggingface"
config:
  model_name: "Qwen/Qwen2-7B-Instruct"
  device_map: "auto"  # Will distribute across available GPUs
  use_fp16: true
  use_4bit: true
  use_8bit: false      # Enable 8-bit quantization for VRAM efficiency
  max_length: 2048
  temperature: 0.1
  top_p: 0.3
  repetition_penalty: 1.1
  cache_dir: "data/model_cache"
  offload_folder: "offload_folder"
  enable_offloading: true
  low_cpu_mem_usage: true
  torch_dtype: "float16"
