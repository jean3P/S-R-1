id: "qwen_32b"
type: "huggingface"
config:
  repo_id: "Qwen/Qwen2.5-32B"
  model_name: "Qwen/Qwen2.5-32B"
  device_map: "auto"  # Will distribute across available GPUs
  use_fp16: true
  use_4bit: true
  use_8bit: false
  max_length: 4096
  temperature: 0.1
  top_p: 0.3
  repetition_penalty: 1.1
  cache_dir: "data/model_cache"
  offload_folder: "offload_folder"
  enable_offloading: true
  low_cpu_mem_usage: true
  torch_dtype: "float16"
