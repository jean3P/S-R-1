id: "qwq_preview"
type: "huggingface"
config:
  model_name: "Qwen/QwQ-32B-Preview"
  device_map: "auto"
  use_fp16: true        # Enable FP16 for memory efficiency
  use_8bit: true        # Enable 8-bit quantization
  use_4bit: true        # Enable 4-bit quantization for extreme memory efficiency
  bnb_4bit_quant_type: "nf4"  # More accurate 4-bit data type
  bnb_4bit_compute_dtype: "float16"  # Compute in FP16
  max_length: 2048
  temperature: 0.2
  top_p: 0.9
  repetition_penalty: 1.1
  cache_dir: "data/model_cache"
  offload_folder: "offload_folder"
  enable_offloading: true
  low_cpu_mem_usage: true
  attn_implementation: "flash_attention_2"  # Use more efficient attention
  torch_dtype: "float16"  # Specify torch dtype explicitly
