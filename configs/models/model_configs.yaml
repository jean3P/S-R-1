# Model configurations for different LLMs

deepseek-32b:
  repo_id: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
  revision: "main"
  trust_remote_code: true
  prompt_template: "<｜begin▁of▁sentence｜>\nHuman: {instruction}\n\nAssistant:"
  quantization:
    bits: 4
    double_quant: true
    quant_type: "nf4"
  use_flash_attention: true

qwen-32b:
  repo_id: "Qwen/Qwen-32B"
  revision: "main"
  trust_remote_code: true
  prompt_template: "<|im_start|>system\nYou are a helpful AI assistant.\n<|im_end|>\n<|im_start|>user\n{instruction}\n<|im_end|>\n<|im_start|>assistant\n"
  quantization:
    bits: 4
    double_quant: true
    quant_type: "nf4"
  use_flash_attention: true

qwq-32b:
  repo_id: "Qwen/QwQ-32B-Preview"
  revision: "main"
  trust_remote_code: true
  prompt_template: "<|im_start|>system\nYou are a helpful AI assistant.\n<|im_end|>\n<|im_start|>user\n{instruction}\n<|im_end|>\n<|im_start|>assistant\n"
  quantization:
    bits: 4
    double_quant: true
    quant_type: "nf4"
  use_flash_attention: true

qwen-coder-7b:
  repo_id: "Qwen/Qwen-7B-Coder"
  revision: "main"
  trust_remote_code: true
  prompt_template: "<|im_start|>system\nYou are a helpful AI assistant skilled in coding.\n<|im_end|>\n<|im_start|>user\n{instruction}\n<|im_end|>\n<|im_start|>assistant\n"
  quantization:
    bits: 4
    double_quant: true
    quant_type: "nf4"
  use_flash_attention: true
